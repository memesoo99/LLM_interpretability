{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2040f6a1-3137-4f6e-b502-72894c7f039e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from datetime import datetime\n",
    "from SAEModel import SparseAutoencoder\n",
    "from torch.nn import functional as F\n",
    "import json\n",
    "\n",
    "input_dim = 3072  # Input and output dimensions\n",
    "hidden_dim = 3072*10  # Hidden layer dimension\n",
    "model = SparseAutoencoder(input_dim, hidden_dim, k = 128, dead_steps_threshold=1000000) # initial lambda is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bc774fbf-72cb-47c8-886d-81ac82d458c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# state_dict = torch.load(\"./experiments/K_128_20241202/models/model_epoch_40.pt\")\n",
    "state_dict = torch.load(\"./models_working_1/model_epoch_34.pt\")\n",
    "# Load the weights into the model\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "68bab8bb-d180-4e0a-a781-d03fe2936a72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseAutoencoder()"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b1b2f2-b8ed-4bff-a2ad-d617858bc455",
   "metadata": {},
   "source": [
    "## Modified Dataset to also return text with embedding for ease of visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "094ad569-bc1b-4330-bf23-84f375b3abea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, path, file_pattern, use_files = 40):\n",
    "        # Load all `.pt` files based on the pattern\n",
    "        self.file_path = path\n",
    "        self.files = sorted(glob.glob(path+\"/\"+file_pattern))\n",
    "        print(f\"Num of files found : {len(self.files)}\")\n",
    "        print(f\"Num of files used : {len(self.files[:use_files])}\")\n",
    "        self.data = []\n",
    "\n",
    "        # Read and store all embeddings from all files\n",
    "        for file in self.files[:use_files]:\n",
    "            batch_data = torch.load(file)\n",
    "            # Extract embeddings and flatten them into a list\n",
    "            self.data.extend([(item[\"embedding\"], item[\"text\"]) for item in batch_data])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ef464f-5e9e-49da-86bd-db16f0fe9fd3",
   "metadata": {},
   "source": [
    "### Checking with 2 books data, to see if their dictionaries capture their main idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b0e9ee12-eca7-4a4a-98ba-29830caa1870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of files found : 1\n",
      "Num of files used : 1\n",
      "Num of files found : 1\n",
      "Num of files used : 1\n"
     ]
    }
   ],
   "source": [
    "path = \"./dataset/testing\"\n",
    "file_pattern = \"p*.pt\"  # Adjust the path if needed\n",
    "prideAndPred = EmbeddingDataset(path, file_pattern , use_files=20)\n",
    "\n",
    "path = \"./dataset/testing\"\n",
    "file_pattern = \"t*.pt\"  # Adjust the path if needed\n",
    "tomSawyer = EmbeddingDataset(path, file_pattern , use_files=20)\n",
    "\n",
    "\n",
    "prideAndPred_loader = DataLoader(prideAndPred, batch_size=1024, shuffle=True)\n",
    "tomSawyer_loader = DataLoader(tomSawyer, batch_size=1024, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ce6af585-6803-4a94-a939-8b66fbde8d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 1.5156, -0.8906, -0.3945,  ..., -0.3770, -0.1543,  1.9844],\n",
       "        dtype=torch.bfloat16),\n",
       " 'In')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea6c896-d36a-4aeb-8f06-867442ed8387",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDict(loader):\n",
    "    word_active_neurons = {}\n",
    "    with torch.no_grad():\n",
    "        for batch_num, batch in tqdm(enumerate(loader), total = len(loader), desc=\"Processing Batches\"):\n",
    "            # Batch to GPU\n",
    "            batch, text = batch\n",
    "            batch = batch.cuda().to(torch.float32)\n",
    "\n",
    "            # Perform forward pass to get latent representations\n",
    "            recons, auxk, num_dead, latents = model(batch)  # Assuming `model` returns latent as the second output\n",
    "\n",
    "            # Find top K active neuron indices for each input in the batch\n",
    "            K = 10  # Assuming TopK SAE is configured with K active neurons\n",
    "            active_neuron_indices = torch.topk(latents, 128, dim=1).indices\n",
    "\n",
    "            # Save results\n",
    "            for i, indices in enumerate(active_neuron_indices):\n",
    "                word = text[i]  # Assuming dataset has words or identifiers\n",
    "                word_active_neurons[word] = indices.cpu().tolist()\n",
    "\n",
    "            if batch_num % 100 == 0:\n",
    "                print(f\"Processed Batch {batch_num}...\")\n",
    "    return word_active_neurons\n",
    "\n",
    "# # Save the mapping to a file\n",
    "# with open(\"word_active_neurons.json\", \"w\") as f:\n",
    "#     json.dump(word_active_neurons, f)\n",
    "\n",
    "# print(\"Forward pass completed and saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be39c910-fd0f-4128-8583-c484be410513",
   "metadata": {},
   "outputs": [],
   "source": [
    "tomSawyerDict = getDict(tomSawyer_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78d54e5-7001-4607-bda8-45fa2357b079",
   "metadata": {},
   "outputs": [],
   "source": [
    "prideAndPredDict = getDict(prideAndPred_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f7166b-e395-48ba-9649-c480c469ad19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def most_common_neurons(neuron_dict, K):\n",
    "    # Flatten all indices into a single list\n",
    "    all_indices = [index for indices in neuron_dict.values() for index in indices]\n",
    "    \n",
    "    # Count the frequency of each index\n",
    "    index_counts = Counter(all_indices)\n",
    "    \n",
    "    # Sort by frequency (descending) and then by index (ascending)\n",
    "    sorted_indices = sorted(index_counts.items(), key=lambda x: (-x[1], x[0]))\n",
    "    \n",
    "    # Return the top K indices\n",
    "    return [index for index, _ in sorted_indices[:K]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c859ec2d-4d56-4cd2-a5c8-a57f29b5f618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_list(index):\n",
    "    word_list = []\n",
    "    for key, val in word_active_neurons.items():\n",
    "        if index in val:\n",
    "            word_list.append(key)\n",
    "    return word_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabdc773-b6f5-4d3e-9996-856ab626ac09",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_word_list(18326))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24f45c2-6193-4b3a-aed1-625dab6fe0fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c8071b-bfe9-4755-b5e5-c9f6f6131a90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92cb31ee-13ab-40c4-8d0a-5ad8e37dec59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of files found : 65\n",
      "Num of files used : 20\n"
     ]
    }
   ],
   "source": [
    "# path = \"./dataset/\"\n",
    "# file_pattern = \"residual_data_batch_*.pt\"  # Adjust the path if needed\n",
    "# embedding_dataset = EmbeddingDataset(path, file_pattern , use_files=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a436a0e-defc-457e-a3d0-2313f6fff90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = DataLoader(embedding_dataset, batch_size=1024, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1eb0eef0-5844-4b8b-b2d2-c4651ba3a0cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseAutoencoder()"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "06973ea1-7ec4-431c-9c93-4644dd19ef49",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e7c0aa44-f17e-4a5f-87ee-2a38cf0e99f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "83393e5d-786d-4f6b-9ea6-74650044a8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:   0%|          | 2/1086 [00:00<04:08,  4.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Batch 0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:   9%|▉         | 102/1086 [00:18<02:56,  5.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Batch 100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  19%|█▊        | 202/1086 [00:36<02:41,  5.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Batch 200...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  28%|██▊       | 302/1086 [00:55<02:29,  5.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Batch 300...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  37%|███▋      | 402/1086 [01:14<02:08,  5.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Batch 400...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  46%|████▌     | 502/1086 [01:32<01:46,  5.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Batch 500...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  55%|█████▌    | 602/1086 [01:50<01:28,  5.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Batch 600...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  65%|██████▍   | 702/1086 [02:09<01:10,  5.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Batch 700...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  74%|███████▍  | 802/1086 [02:27<00:53,  5.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Batch 800...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  83%|████████▎ | 902/1086 [02:46<00:34,  5.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Batch 900...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  92%|█████████▏| 1002/1086 [03:05<00:15,  5.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Batch 1000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|██████████| 1086/1086 [03:20<00:00,  5.42it/s]\n"
     ]
    }
   ],
   "source": [
    "# word_active_neurons = {}\n",
    "# with torch.no_grad():\n",
    "#     for batch_num, batch in tqdm(enumerate(train_loader), total = len(train_loader), desc=\"Processing Batches\"):\n",
    "#         # Batch to GPU\n",
    "#         batch, text = batch\n",
    "#         batch = batch.cuda().to(torch.float32)\n",
    "        \n",
    "#         # Perform forward pass to get latent representations\n",
    "#         recons, auxk, num_dead, latents = model(batch)  # Assuming `model` returns latent as the second output\n",
    "        \n",
    "#         # Find top K active neuron indices for each input in the batch\n",
    "#         K = 10  # Assuming TopK SAE is configured with K active neurons\n",
    "#         active_neuron_indices = torch.topk(latents, 128, dim=1).indices\n",
    "        \n",
    "#         # Save results\n",
    "#         for i, indices in enumerate(active_neuron_indices):\n",
    "#             word = text[i]  # Assuming dataset has words or identifiers\n",
    "#             word_active_neurons[word] = indices.cpu().tolist()\n",
    "            \n",
    "#         if batch_num % 100 == 0:\n",
    "#             print(f\"Processed Batch {batch_num}...\")\n",
    "\n",
    "# # # Save the mapping to a file\n",
    "# # with open(\"word_active_neurons.json\", \"w\") as f:\n",
    "# #     json.dump(word_active_neurons, f)\n",
    "\n",
    "# # print(\"Forward pass completed and saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79b4574-a238-4518-9a84-d6e5827fbc70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3727223b-6a9e-4939-b282-7b76ffddfaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"word_active_neurons_2Dec.json\", \"w\") as f:\n",
    "    json.dump(word_active_neurons, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a551b57d-fcda-4f68-b0d2-276c3609a4ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "949de672-b325-41a8-a00e-e5cd592e3377",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(word_active_neurons, f\"./output_2Dec.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e3613c8-c3b6-4076-95b6-091faf022675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to ./experiments/K_128_20241202/word_active_neurons.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "csv_file = './experiments/K_128_20241202/word_active_neurons.csv'\n",
    "\n",
    "# Open the CSV file for writing\n",
    "with open(csv_file, mode='w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    # Write the header (optional, for clarity)\n",
    "    header = ['word'] + [f'num{i+1}' for i in range(128)]  # Generate column names like num1, num2, ...\n",
    "    writer.writerow(header)\n",
    "    \n",
    "    # Write each word and its corresponding list of 128 numbers\n",
    "    for word, numbers in word_active_neurons.items():\n",
    "        writer.writerow([word] + numbers)\n",
    "\n",
    "print(f\"Data successfully saved to {csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4e6beb-78e6-41b5-99d3-bddf34a2f97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_list(index):\n",
    "    word_list = []\n",
    "    for key, val in word_active_neurons.items():\n",
    "        if index in val:\n",
    "            word_list.append(key)\n",
    "    return word_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6a465420-cb6a-4078-95ae-83b672d1916e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eth', 'King', 'Andy', 'ey', 'Fin', 'ack', 'dy', 'ilda', 'Elizabeth', 'Guy', 'gers', 'speaker', 'Lion', 'Rome', 'Mario', 'prince', 'icus', 'host', 'Castro', 'assen', 'Jack', 'iver', 'Martin', 'ome', 'Spain', 'Rose', 'Han', 'Susan', 'Napoleon', 'alf', 'rah', 'enger', 'vie', 'Wolf', 'Ron', 'Haz', 'ku', 'Harris', 'Matrix', 'uzz', 'Pi', 'Beth', 'Lincoln', 'etta', 'ione', 'Duke', 'Ruth', 'Bull', 'Ash', 'RED', 'Robinson', 'iki', 'olas', 'ierre', 'obe', 'ala', 'ocket', 'Lan', 'oco', 'Russell', 'Jesus', 'gang', 'nik', 'Arag', 'boys', 'edi', 'iley', 'Wes', 'Herbert', 'Neil', 'ario', 'Nick', 'nier', 'XVI', 'Ministry', 'etto', 'hof', 'Farm', 'anos', 'Elli', 'Bruce', 'Committee', 'Luigi', 'Bayern']\n"
     ]
    }
   ],
   "source": [
    "print(get_word_list(0)) #Names?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1d5e0b31-3416-4f72-8333-0c55211ff118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cludes', 'oses', 'decided', 'commit', 'figured', 'promised', 'determined', 'ered', 'agreed', 'decide', 'wanted', 'cide', 'dedicated', 'managed', 'chose', 'convinced', 'arks', 'concluded', 'attempted', 'switched', 'decid', 'Figure', 'refuse']\n"
     ]
    }
   ],
   "source": [
    "print(get_word_list(1)) # Verbs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1ca983ed-5495-4b7d-af45-ad61277b5bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['King', 'al', 'Cris', 'Swift', 'dy', 'Lee', 'ilda', 'ingo', 'Hamilton', 'itch', 'Korea', 'idas', 'iden', 'boards', 'BI', 'rost', 'ier', 'Jul', 'icus', 'Ferdinand', 'ko', 'iet', 'Taylor', 'ads', 'assen', 'bage', 'ER', 'Kelly', 'Night', 'Germ', 'Wolf', 'inci', 'hart', 'ource', 'Pi', 'etta', 'Adam', 'Duke', 'horses', 'Bull', 'Robinson', 'anto', 'olan', 'ierre', 'inek', 'Moore', 'PA', 'Lucas', 'Budapest', 'ima', 'ocket', 'ange', 'Gilbert', 'chev', 'oku', 'fold', 'ga', 'nik', 'Factory', 'jin', 'iley', 'elin', 'Alert', 'Columb', 'baum', 'urst', 'ensen', 'owski', 'uffer', 'boa', 'hof', 'anos', 'Inn', 'eny', 'adi', 'ritz', 'Pitts']\n"
     ]
    }
   ],
   "source": [
    "print(get_word_list(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f96e3f49-2476-455b-a316-46fc4000d6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['benefits', 'rising', 'rapidly', 'quick', 'faster', 'massive', 'entially', 'evol', 'gent', 'atic', 'Rap', 'sudden', 'ancement', 'rapid', 'mediate', 'exponential', 'racing', 'rá']\n"
     ]
    }
   ],
   "source": [
    "print(get_word_list(4))  # Rapid and instantly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "50d9dff2-1e9d-4dbd-adb5-486a8175bf38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['King', 'al', 'Cris', 'Swift', 'dy', 'Lee', 'ilda', 'ingo', 'Hamilton', 'itch', 'Korea', 'idas', 'iden', 'boards', 'BI', 'rost', 'ier', 'Jul', 'icus', 'Ferdinand', 'ko', 'iet', 'Taylor', 'ads', 'assen', 'bage', 'ER', 'Kelly', 'Night', 'Germ', 'Wolf', 'inci', 'hart', 'ource', 'Pi', 'etta', 'Adam', 'Duke', 'horses', 'Bull', 'Robinson', 'anto', 'olan', 'ierre', 'inek', 'Moore', 'PA', 'Lucas', 'Budapest', 'ima', 'ocket', 'ange', 'Gilbert', 'chev', 'oku', 'fold', 'ga', 'nik', 'Factory', 'jin', 'iley', 'elin', 'Alert', 'Columb', 'baum', 'urst', 'ensen', 'owski', 'uffer', 'boa', 'hof', 'anos', 'Inn', 'eny', 'adi', 'ritz', 'Pitts']\n",
      "\n",
      "['benefits', 'rising', 'rapidly', 'quick', 'faster', 'massive', 'entially', 'evol', 'gent', 'atic', 'Rap', 'sudden', 'ancement', 'rapid', 'mediate', 'exponential', 'racing', 'rá']\n",
      "\n",
      "['bow', 'rob', 'cc', 'elect', 'produ', 'ester', 'hosp', 'lad', 'igare', 'Rou', 'rou']\n",
      "\n",
      "['Random', 'eth', 'Food', 'different', 'ately', 'String', 'first', 'element', 'remove', 'char', 'oder', 'walls', 'onic', 'Perm', 'min', 'publication', 'colors', 'phrase', 'volution', 'awa', 'Mountain', 'Builder', '(\"', 'Node', 'const', 'Chinese', 'finally', 'Saf', 'han', '£', 'exist', 'wikipedia', 'lyn', 'host', 'ratio', 'nem', 'populated', 'gun', 'Math', '::', ')(', 'alphabet', 'cipe', 'Mi', 'Hit', 'subs', 'Better', 'Wolf', '€', 'percent', '(!', 'calcul', 'combinations', 'anto', 'sizeof', '![', 'statue', 'Fox', 'permut', 'ère', 'null', 'bool', 'Until', 'endo', 'ategories', 'ArrayList', 'docker', 'Mason', 'MIN', 'ilus', ':#', 'sche', 'ithub', 'DECLARE', 'trois']\n",
      "\n",
      "['endency', 'train', 'constitution', 'ist', 'recognition', 'modern', 'networks', 'comput', 'ota', 'software', 'ect', 'painter', 'iction', 'computers', 'oration', 'ural', 'commercial', 'ian', 'ilation', 'ix', 'fiction', 'Bible', 'NN', 'philosophy', 'webpage', 'istics', 'iller', 'ometry', 'benchmark', 'addresses', 'punk', 'oles', 'metric', 'Distribution', 'Muslim', 'Bib', 'oux', 'NFL', 'Testament']\n",
      "\n",
      "['s', 'oses', 'person', 'own', 'houses', 'essed', 'between', 'computer', 'ctors', 'et', 'cultural', 'stronger', 'Auth', 'developing', 'ured', 'tool', 'breaks', 'pride', 'historical', 'wood', 'Research', 'Text', 'Business', 'engine', 'Illinois', 'omin', 'operations', 'secre', 'prom', 'behav', 'ensive', 'mission', 'Well', 'Psych', 'advice', 'establish', 'green', 'attitude', 'worker', 'himself', 'idas', 'efficiently', 'platform', 'bott', 'ulate', 'structure', 'Technology', 'physics', 'terrible', 'released', 'oy', 'websites', 'Em', 'influ', 'bridge', 'prem', 'Tokyo', 'surv', 'readable', 'clause', 'eral', 'gli', 'served', 'np', 'omena', 'Bird', 'Spain', 'procedures', 'mainly', 'arrow', 'metadata', 'het', 'stable', 'rows', 'Mobile', 'Australian', 'Production', 'authentication', 'teachers', 'mod', 'Run', 'promises', 'forum', 'Bor', 'ani', 'promoted', 'emed', 'alo', 'centre', 'Georgia', 'Docker', 'displaying', 'boundary', 'southern', 'oj', 'cussion', 'essions', 'Argument', 'istance', 'acters', 'ebook', 'Guinea', 'onial', 'edule', 'Colors', 'Government', 'Catal', 'suic', 'Was', 'mas', 'Optional', 'istas', 'aterial', 'enst', 'sunt', 'Harm', 'Kap', '\"],', 'DK', 'Kas', 'namely', 'posted', 'roller', 'LS', 'Rib', 'Him', 'Staff']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in [2,4,5,10,15,21]:\n",
    "    print(get_word_list(i))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0a54d478-98e6-454b-8a58-6792c2ace4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDict(loader):\n",
    "    word_active_neurons = {}\n",
    "    with torch.no_grad():\n",
    "        for batch_num, batch in tqdm(enumerate(loader), total = len(loader), desc=\"Processing Batches\"):\n",
    "            # Batch to GPU\n",
    "            batch, text = batch\n",
    "            batch = batch.cuda().to(torch.float32)\n",
    "\n",
    "            # Perform forward pass to get latent representations\n",
    "            recons, auxk, num_dead, latents = model(batch)  # Assuming `model` returns latent as the second output\n",
    "\n",
    "            # Find top K active neuron indices for each input in the batch\n",
    "            K = 10  # Assuming TopK SAE is configured with K active neurons\n",
    "            active_neuron_indices = torch.topk(latents, 128, dim=1).indices\n",
    "\n",
    "            # Save results\n",
    "            for i, indices in enumerate(active_neuron_indices):\n",
    "                word = text[i]  # Assuming dataset has words or identifiers\n",
    "                word_active_neurons[word] = indices.cpu().tolist()\n",
    "\n",
    "            if batch_num % 100 == 0:\n",
    "                print(f\"Processed Batch {batch_num}...\")\n",
    "    return word_active_neurons\n",
    "\n",
    "# # Save the mapping to a file\n",
    "# with open(\"word_active_neurons.json\", \"w\") as f:\n",
    "#     json.dump(word_active_neurons, f)\n",
    "\n",
    "# print(\"Forward pass completed and saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc71634-daa8-4d5b-90e9-171ed9d47bee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bf210edb-be54-4adc-80aa-b687e70f12ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|██████████| 2/2 [00:00<00:00,  6.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Batch 0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tomSawyerDict = getDict(tomSawyer_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2e7afdf2-db69-4eb5-ab60-f0f9530a1d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|██████████| 2/2 [00:00<00:00,  8.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Batch 0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prideAndPredDict = getDict(prideAndPred_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f8f88c6d-8b3a-41dc-87c3-741eec0833db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def most_common_neurons(neuron_dict, K):\n",
    "    # Flatten all indices into a single list\n",
    "    all_indices = [index for indices in neuron_dict.values() for index in indices]\n",
    "    \n",
    "    # Count the frequency of each index\n",
    "    index_counts = Counter(all_indices)\n",
    "    \n",
    "    # Sort by frequency (descending) and then by index (ascending)\n",
    "    sorted_indices = sorted(index_counts.items(), key=lambda x: (-x[1], x[0]))\n",
    "    \n",
    "    # Return the top K indices\n",
    "    return [index for index, _ in sorted_indices[:K]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4c8a879f-328a-4b8d-9e4d-7f56a006b82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tomSawyerDict_themes = most_common_neurons(tomSawyerDict, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bd743e9a-89de-46e6-b8bc-9b390c6b08bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7409, 15362, 15271, 18441, 17306, 7587, 6103, 16430, 10140, 18326]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tomSawyerDict_themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "99b0b07f-d6f4-441c-85be-d37500935455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13124\n",
      "13126\n",
      "13049\n",
      "1509\n",
      "12530\n",
      "717\n",
      "5571\n",
      "4339\n",
      "3433\n",
      "385\n"
     ]
    }
   ],
   "source": [
    "for i in tomSawyerDict_themes:\n",
    "    print(len(get_word_list(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d6e550e1-72bf-4e9d-b08f-dc64f440a969",
   "metadata": {},
   "outputs": [],
   "source": [
    "prideAndPred_themes = most_common_neurons(prideAndPredDict, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4c41b979-76d6-497d-ac2e-e8b85f859346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7409, 15271, 15362, 17306, 18441, 7587, 6103, 23704, 16430, 3908]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prideAndPred_themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a4fa88bb-36ff-4101-8419-f95a2f6cea68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13124\n",
      "13049\n",
      "13126\n",
      "12530\n",
      "1509\n",
      "717\n",
      "5571\n",
      "9828\n",
      "4339\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "for i in prideAndPred_themes:\n",
    "    print(len(get_word_list(i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ded5b5-cea8-4201-a2ae-06ecae412176",
   "metadata": {},
   "source": [
    "### For Tom Sawyer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fa286454-2983-446a-8a99-937035c41c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', 'pe', 'me', 'She', 'on', 'come', 'closer', 'I', 'she', 'found', 'proper', 'flav', 'still', 'immediately', 'will', 'ill', 'been', 'light', 'll', 'great', 'noticed', 'never', 'lines', 'thus', 'With', 'values', 'instead', 'he', 'something', 'home', 'live', 'y', 'even', 'itself', 'hur', 'escape', 'aim', 'Whether', 'try', 'filled', 'had', 'releases', 'secret', 'before', 'decided', 'Mark', 'bb', 'island', 'reduce', 'chance', 'builds', 'looks', 'im', 'achieve', 'journey', 'get', 'make', 'heard', 'amaz', 'dropped', 'susp', 'remain', 'reve', 'saf', 'seeking', 'tail', 'knows', 'enough', 'lived', 'ished', 'bring', 'discovered', 'edge', 'fun', 'ge', 'leaving', 'det', 'Tr', 'holding', 'bag', 'led', 'ilt', 'per', 'acles', 'further', 'remember', 'hill', 'figured', 'admitted', 'remembered', 'takes', 'heavy', 'Every', 'wonder', 'perfect', 'forces', 'happy', 'amount', 'probably', 'did', 'alone', '.”', 'secre', 'hard', 'den', 'ky', 'soft', 'deliver', 'reached', 'lied', 'wel', 'mission', 'hands', 'got', 'wounded', 'defeat', 'itting', 'few', 'satisf', 'approached', 'letting', '\">', 'clever', 'Cel', 'que', 'cur', 'massive', 'bra', 'promised', 'himself', 'determined', 'sword', 'couple', 'particip', 'too', 'killed', 'brace', 'facing', 'ered', 'acc', 'yet', 'says', 'march', 'backing', 'inspired', 'oby', 'ere', 'folk', 'relief', 'experiments', 'brilliant', 'sent', 'stepped', 'attempts', 'hel', 'aria', 'reporting', 'Rome', 'partner', 'returned', 'received', 'arrived', 'passing', 'hes', 'incorpor', 'straight', 'ric', 'ark', 'token', 'drove', 'clearer', 'inserted', 'watched', 'rayed', 'behind', 'devoted', ',”', 'watching', 'wanted', 'Sarah', 'retre', 'fallen', 'Mr', 'doub', 'Pe', 'loved', 'umbled', 'shock', 'smiled', 'fate', 'request', 'caught', 'recovered', 'Show', 'touched', 'leave', 'runs', 'seek', 'iet', 'promise', 'became', 'careful', 'bigger', 'matching', 'whom', 'cess', 'redis', 'ido', 'disappeared', 'managed', 'pos', 'tiny', 'came', 'tokens', 'wet', 'carry', 'fails', 'Did', 'proves', 'vi', 'defeated', 'ounce', 'plates', 'unge', 'bother', 'fighting', 'hid', 'returning', 'Before', 'entered', 'hearts', 'upper', 'Sym', 'listener', 'watch', 'Real', 'tested', 'bless', 'declared', 'Grace', 'grant', 'mitt', 'Okay', 'aston', 'gaz', 'faint', 'chose', 'Upon', 'encounter', 'theless', 'horse', 'thinks', 'describing', 'Years', 'appeared', 'iest', 'dim', 'asks', 'applicable', 'Grad', 'hunting', 'tan', 'turned', 'unted', 'ati', 'gradually', 'anymore', 'Ir', 'Now', 'opport', 'entin', 'native', 'arrival', 'Haz', 'rescue', 'iously', 'originally', 'Initial', 'truth', 'merely', 'revealed', 'Tennessee', 'deser', 'accepts', 'package', 'shoot', 'percent', 'advise', 'surpr', 'grounds', 'convinced', 'inue', 'whisper', 'arks', 'alleg', 'shout', 'eff', 'stops', 'somehow', 'intu', 'hardly', 'Jimmy', 'faithful', 'dear', 'stark', 'chosen', 'heading', 'acquired', 'escaped', 'tack', 'rab', 'bitter', 'ntil', 'officer', 'mediate', 'commits', 'crossed', 'avoided', 'asking', 'Iowa', 'arose', 'existence', 'maybe', 'cile', 'confl', 'Gregory', '?”', 'rix', 'hurt', 'Af', 'fatal', 'icion', 'suffered', 'mom', 'Palest', 'tries', 'cia', 'certainly', 'fled', 'Got', 'claimed', 'Was', 'proc', 'prize', 'fortun', 'initially', 'athed', 'zed', 'attempted', 'sooner', 'explained', 'sli', 'hadn', 'joins', 'wore', 'leases', 'Und', 'pulling', '‐', 'esc', 'haps', 'durante', 'otal', 'soldier', 'tempt', 'aug', 'pointed', 'grave', 'udes', 'persist', 'obs', 'awful', 'vom', 'estaba', 'pac', 'anyway', 'fleet', 'uder', 'disapp', '-$', 'conced', 'somewhat', 'kill', 'ulp', 'aunque']\n"
     ]
    }
   ],
   "source": [
    "print(get_word_list(18326))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1ae033-7ecf-4b51-87ad-568dc6895be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The overarching theme of this set of words appears to revolve around journeys, encounters, and emotional states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77fcbe9-c714-4370-aef9-e7749a5bd1d5",
   "metadata": {},
   "source": [
    "### For pride and Prejudice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a09cdaa8-285e-4608-be43-6146a162e5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['connected', 'King', 'German', 'couple', 'idas', 'hat', 'shall', 'Rome', 'acher', 'prince', 'Jul', 'iet', 'whom', 'marry', 'ome', 'tested', 'asks', 'honor', 'convinced', 'faithful', 'duty', 'commits', 'trag', 'recon', 'Father', 'cile', 'fore', 'thou', 'conflic', 'Honor']\n"
     ]
    }
   ],
   "source": [
    "print(get_word_list(3908))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c20b542b-4483-486e-a121-2a8dfdbca114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The theme of this collection centers on duty, honor, and interpersonal relationships, often framed in a historical or romantic context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99f0425-fd86-4c13-87ea-ea954c7599c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1cc68dc3-1ef0-4396-9354-4efbb7a33011",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def most_common_neurons(neuron_dict, K):\n",
    "    # Flatten all indices into a single list\n",
    "    all_indices = [index for indices in neuron_dict.values() for index in indices]\n",
    "    \n",
    "    # Count the frequency of each index\n",
    "    index_counts = Counter(all_indices)\n",
    "    \n",
    "    # Sort by frequency (descending) and then by index (ascending)\n",
    "    sorted_indices = sorted(index_counts.items(), key=lambda x: (-x[1], x[0]))\n",
    "    \n",
    "    # Return the top K indices\n",
    "    return [index for index, _ in sorted_indices[K:K+K]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2ff1e680-353c-41cf-a4c4-358279ac99aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tomSawyerDict_themes = most_common_neurons(tomSawyerDict, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d8d2a81d-85fa-44ba-a77e-a41a4057b340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[23704, 25660, 2948, 18538, 15144, 7788, 28030, 20709, 19313, 11838]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tomSawyerDict_themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2effc356-91bc-4bd9-a3b6-2d925e952a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuron Num, 23704, Len of its dict 9828\n",
      "Neuron Num, 25660, Len of its dict 7041\n",
      "Neuron Num, 2948, Len of its dict 9199\n",
      "Neuron Num, 18538, Len of its dict 147\n",
      "Neuron Num, 15144, Len of its dict 2495\n",
      "Neuron Num, 7788, Len of its dict 319\n",
      "Neuron Num, 28030, Len of its dict 75\n",
      "Neuron Num, 20709, Len of its dict 2645\n",
      "Neuron Num, 19313, Len of its dict 985\n",
      "Neuron Num, 11838, Len of its dict 2933\n"
     ]
    }
   ],
   "source": [
    "for i in tomSawyerDict_themes:\n",
    "    print(f\"Neuron Num, {i}, Len of its dict {len(get_word_list(i))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b70f8b25-f46e-4d98-a552-e551b9a66373",
   "metadata": {},
   "outputs": [],
   "source": [
    "prideAndPred_themes = most_common_neurons(prideAndPredDict, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "67aec0d1-87d8-4adb-9071-be18769ef8dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7788, 25660, 18538, 2948, 18326, 10140, 9179, 20709, 11838, 15144]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prideAndPred_themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "fd185356-fc63-4aca-adbf-91c97e6ae56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuron Num, 7788, Len of its dict 319\n",
      "Neuron Num, 25660, Len of its dict 7041\n",
      "Neuron Num, 18538, Len of its dict 147\n",
      "Neuron Num, 2948, Len of its dict 9199\n",
      "Neuron Num, 18326, Len of its dict 385\n",
      "Neuron Num, 10140, Len of its dict 3433\n",
      "Neuron Num, 9179, Len of its dict 59\n",
      "Neuron Num, 20709, Len of its dict 2645\n",
      "Neuron Num, 11838, Len of its dict 2933\n",
      "Neuron Num, 15144, Len of its dict 2495\n"
     ]
    }
   ],
   "source": [
    "for i in prideAndPred_themes:\n",
    "    print(f\"Neuron Num, {i}, Len of its dict {len(get_word_list(i))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2422e887-53d1-4679-8cd4-a6478418f5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cre', 'come', 'eth', 'ones', 'ians', 'kind', 'ever', 'instead', 'human', 'regime', 'live', 'even', 'secret', 'looks', 'heard', 'ole', 'neighbourhood', 'ank', 'river', 'judge', 'drink', 'bag', 'ilt', 'remember', 'hill', 'takes', 'ships', 'lack', '—', 'wel', 'mission', 'interrupt', 'itch', 'unn', 'strugg', 'ief', 'lif', 'males', 'killed', 'speaker', 'inspired', 'employee', 'resource', 'construct', 'shall', 'Rome', 'violent', 'asion', 'shell', 'icked', 'party', 'ric', 'acher', 'wid', 'prince', 'voyage', 'batt', 'elf', 'request', 'soup', 'white', 'idden', 'iet', 'compan', 'bomb', 'cons', 'whom', 'ury', 'itan', 'fails', 'marry', 'reb', 'returning', 'queen', 'Oregon', 'Upon', 'opening', 'brothers', 'conscience', 'Years', 'appeared', 'itution', 'brid', 'sheep', 'painted', 'gradually', 'Wolf', 'agger', 'originally', 'army', 'ego', 'boats', 'villa', 'convinced', 'elder', 'sib', 'faithful', 'anda', 'cres', 'wife', 'Mississippi', 'ruled', 'Monsieur', 'commits', 'forcing', 'uncle', 'tribes', 'Lan', 'hurt', 'tries', 'Sou', 'poet', 'collision', 'fri', 'Dark', 'riors', 'boys', 'extr', 'urk', 'urd', 'joins', 'iley', 'Swiss', 'elaborate', 'Herbert', 'milit', 'sons', 'accus', 'elin', 'igan', 'finale', 'Ministry', 'Chief', 'emor', 'spectral', 'umably', 'chief', 'corps', 'troops', 'voy', 'bian', 'slash', 'Elli', 'umed', 'kill', 'nep', 'terminal', '(', 'll', 've', 'spell', 'ids', 'In', 'improved', 'positive', 'friends', 'char', 'lived', 'thre', 'oken', 'append', 'vehicles', 'integral', 'ish', 'ter', 'ess', 'Per', 'sy', 'assistant', '.”', 'Try', ');', 'ende', 'dogs', 'singles', 'spr', '</', 'encies', 'Co', 'Rome', 'People', 'intellig', 'Trump', 'Wall', '§', 'html', 'liberal', 'itos', 'iet', 'Virgin', 'Lu', 'East', 'feas', 'confront', 'adopted', '         ', 'Fran', 'faint', 'citiz', 'Yet', 'Vo', 'Note', 'UC', 'icle', 'hire', 'atile', 'whisper', 'Georgia', 'Nevertheless', 'proved', 'aco', 'Story', 'Msg', 'Submit', 'jour', 'Wor', 'esota', 'bool', 'ingen', 'ATION', 'conclude', 'parsed']\n"
     ]
    }
   ],
   "source": [
    "print(get_word_list(28030))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d31c118-4085-44fc-93b4-f280a276935a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fcbd2e-83e4-4f10-b6e3-37b2cfe566c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bob', 'Richard', 'cave', 'marriage', 'father', 'ocr', 'killed', 'says', 'Mar', 'China', 'ric', 'ier', 'wid', 'Mat', 'prince', 'loved', 'Ferdinand', 'eding', 'whom', 'cess', 'Franz', 'marry', 'Marcus', 'Mrs', 'Jac', 'Parliament', 'Cle', 'arrival', 'Haz', 'villa', 'Gar', 'Austria', 'etta', 'faithful', 'ixon', 'wife', 'cousin', 'relatives', 'Robinson', 'uw', 'uncle', 'ocket', 'Lan', 'assador', 'victim', 'mate', 'Buch', 'Conference', 'anie', 'Herbert', 'accus', 'slave', 'bian', 'Abd', 'adi', 'disapp', 'XVIII', 'nep', 'ouv']\n"
     ]
    }
   ],
   "source": [
    "print(get_word_list(9179))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacfc2d4-59a7-453e-9764-7dc432e2cfd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a655e9d-23db-473e-a135-ed0a03140438",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99fc6a6-4084-42b6-be34-736990322e71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "71359205-a5f6-46dd-b86f-10203a2e4353",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topK_activation(x, k):\n",
    "    topk = torch.topk(x, k=k, dim=-1, sorted=False)\n",
    "    values = F.relu(topk.values)\n",
    "    result = torch.zeros_like(x)\n",
    "    result.scatter_(-1, topk.indices, values)\n",
    "    return topk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "da669b61-163f-46f2-b267-b9fb4fad4122",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn((1, 900))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "85a49a56-108d-4b04-b5ba-c5dd769f14da",
   "metadata": {},
   "outputs": [],
   "source": [
    "topK = topK_activation(a, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "cc07dd1c-144e-44b0-ab75-01667aeab7db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[312, 335, 895, 683, 817,  28, 178,  26, 742, 124]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topK.indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1056ef35-b2a1-4248-882b-5b976a8454f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  1.4375,  0.0000,  0.8863,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000, -0.8961,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.3724,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.7095,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.1425,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0893,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.8492,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  1.0699,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.5379,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = torch.zeros_like(a)\n",
    "result.scatter_(-1, topK.indices, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "93e6372f-6589-4076-9093-e8b8b231ac4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = []\n",
    "for ind in tomSawyerDict_themes:\n",
    "    sent.append(' '.join(word for word in get_word_list(ind)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9b773175-ec2f-496b-bee5-38bb4ada6867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "015532f7-a168-4cd8-b264-6c031f9cc255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(get_word_list(129))  #Fractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca8fac15-3f42-4d43-99c9-f181f927f5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(get_word_list(30718)) # Leadership roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b84d1c5-018c-4678-b168-a2969a9149e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(get_word_list(30717))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a4a16b4-60dd-4f5a-a1fd-7b0e21e9148a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(get_word_list(30710))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9b3c956d-ab13-425f-90d1-025868e2c6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['game', 'played', 'Game', 'games', 'Games', 'sport', 'tery', 'Scene', 'Gam', 'amo']\n"
     ]
    }
   ],
   "source": [
    "print(get_word_list(30709))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc53a0a-fc73-4eca-adc2-d24c85aaf599",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_word_list(30704))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea77ee1e-e594-4417-8416-fe2c8044e248",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98079a2-669c-4279-a711-b3ce16d72d9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd86ede0-89ac-4303-af8a-aa142d1d765a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": ".env",
   "name": "common-cu113.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu113:m112"
  },
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": ".env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
